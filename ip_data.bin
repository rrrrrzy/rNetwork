这是一个测试文段，用于测试我们编写的以太网协议栈与IP协议，写成这样有意义的话语有利于分析程序的运行。
1、本节课任务要求：
（1） 发送方：构造一个IP分组，支持分片，计算简单校验和，并交付给数据链路层（封装成数据帧并发送）；
（2） 接收方：从数据链路层接收到一个IP分组，检查目的IP地址是否匹配，检测校验和；如果分片需要重新组装；
对IP分组首部进行解析并打印，根据协议类型，将IP分组数据部分交付给上层协议（写到一个文件中）。

On Jan. 24, as Chinese prepared to celebrate Spring Festival, Western industry leaders—LLM engineers, investors, and shareholders—endured their "BLACK MONDAY." Days later, NVIDIA, the world’s largest graphics card producer, lost nearly 17% of its stock value. All of this was triggered by the announcement of DeepSeek-R1, a Chinese Large Language Model (LLM). Yet, with many Chinese LLMs already on the market, why did DeepSeek-R1 shock the world? And why did NVIDIA, not Intel or AMD, bear the brunt? It will be explained as follows.
 
Body
I. Main point DeepSeek-R1 distinguishes itself from previous Chinese LLMs as the FIRST to support "deep thinking"              
 Internal preview: This section will explain the technical breakthroughs enabling "deep thinking" and its implications for AI capabilities.
A. Subpoint Introduces a novel "chain-of-thought reasoning" architecture that shows humanlike multi-step problem-solving.
 
B. Subpoint Achieves 3 times longer context retention than GPT-4, enabling complex logical deductions.
 Internal summary: These innovations redefine what LLMs can achieve, pushing AI beyond chatbot capabilities into domains requiring sustained analytical rigor.
 
(Transition: However, the China’s first deep think model as the R1 is, it is still far away to be called the “sputnik time” in AI field. There are more important reasons.)
 
II. Main point: The training methodology for DeepSeek-R1 diverges significantly from conventional approaches, reducing costs by 95% compared to ChatGPT-o1.
 Internal preview: This section will compare traditional vs. DeepSeek-R1's training frameworks and their economic impact.
 A. Subpoint Creation in model itself
1. Sub-subpoint Introduces a novel "chain-of-thought reasoning" architecture that mimics human multi-step problem-solving.
 
2. Sub-subpoint Using Mixture-of-Experts (MoE) architecture, reducing 80% calculation work compared to traditional model(Transformer).
3. Sub-subpoint Open sourced its model parameter, in opposite, OpenAI closed all its model after ChatGPT3.
 B. Subpoint Innovation in algorithm
1. Sub-subpoint Publish a new algorithm -- Group Relative Policy Optimization (GRPO) , costs 40% graphic memory less than traditional Proximal Policy Optimization (PPO)
2. Sub-subpoint Use reinforce learning (RL) to substitute for Supervised fine-tuning (SFT), decreasing the reliance of Human’s work -- data label.
 Internal summary: By slashing training costs and democratizing access to high-performance LLMs, DeepSeek-R1 threatens the oligopoly of Western AI giants.
 
(Transition: Now, I think, the wholly new training method and algorithm, open-sourced model parameter and ending the meaningless and relentless compute arm race can make it the “sputnik time”. But, there is another mystery behind it. That is ...)
 
III. Main point: DeepSeek-R1 eliminates platform-specific dependencies, enabling training on non-NVIDIA GPU.
Internal preview: This section will detail how DeepSeek-R1's hardware-agnostic design disrupts NVIDIA's CUDA monopoly.
 A. Subpoint It bypassed CUDA framework in hardware level, directly using the assembly code
1. Sub-subpoint Developed a universal tensor compiler compatible with AMD, Huawei Ascend, and custom ASIC chips.
2. Sub-subpoint Achieved 90% of NVIDIA's performance on Huawei's Ascend 910 chips through low-level optimization.
 B. Subpoint Open-sourced its hardware abstraction layer (HAL), inviting global developers to contribute drivers.
 Internal summary: By decoupling AI innovation from proprietary hardware ecosystems, DeepSeek-R1 paves the way for a decentralized, geopolitically resilient AI industry.
 
Conclusion
The release of DeepSeek-R1 marks a pivotal milestone in China’s LLM development. It demonstrates that advanced AI can flourish even amid hardware constraints, challenging the perceived supremacy of OpenAI and NVIDIA. By pioneering an alternative path for innovation, DeepSeek-R1 redefines global AI competition and underscores the potential for decentralized technological progress.Just as the original Sputnik moment reshaped global tech rivalry, DeepSeek-R1 signals a new era in AI—one where ingenuity, not infrastructure dominance, defines progress. This is China’s Sputnik Moment: a declaration that the future of LLMs is not dictated by hardware monopolies, but by the courage to rethink what’s possible.
 
Peer review (comments and advise): (此部分由审阅同学填写)
 
Comments: (整体评价)
该大纲结构清晰、逻辑严谨，围绕“DeepSeek-R1为何被称为AI领域的斯普特尼克时刻”这一核心问题展开，层层递进，具有较强的说服力。三大主论点（深度思维能力、训练成本革命、硬件去依赖化）精准抓住了技术突破的关键维度，过渡句自然衔接章节内容，结论部分升华主题，呼应了“斯普特尼克时刻”的历史类比意义。语言风格兼具学术性与可读性，案例选取具体且有冲击力，能够有效支撑论点。
 
 
Advise: (建议)
1.增强数据支撑 ：建议在II.A.2（减少数据标注依赖）和III.A.1（华为Ascend芯片性能对比）中补充具体实验数据或第三方测试报告，以强化技术可信度。
2.扩展社会影响分析 ：结论部分可进一步探讨技术突破对地缘政治、开源生态或伦理监管的潜在影响（如西方对华技术封锁的反作用）。
